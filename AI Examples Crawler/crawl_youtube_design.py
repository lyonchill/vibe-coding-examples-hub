"""
å°ˆé–€çˆ¬å– YouTube ä¸Š 15 å€‹æœ€å—æ­¡è¿çš„è¨­è¨ˆç›¸é—œ vibe-coding æ¡ˆä¾‹
å°ˆæ³¨åœ¨é€é Cursor, Figma Make/MCP ç­‰å·¥å…·å¯¦éš›æ§‹å»º UI/UX design, design system, web design
"""
import json
import sys
from pathlib import Path
from datetime import datetime, timedelta
from dotenv import load_dotenv

sys.path.insert(0, str(Path(__file__).parent))
from ai_examples_crawler import AIExamplesCrawler

load_dotenv()

# å„ªåŒ–å¾Œçš„é—œéµå­—ï¼šç°¡åŒ–ä»¥é¿å… API é™åˆ¶ï¼ŒåŒæ™‚ä¿æŒç²¾ç¢ºæ€§
# é‡é»ï¼šä½¿ç”¨è¼ƒçŸ­çš„é—œéµå­—ï¼Œé¿å…éæ–¼è¤‡é›œçš„æŸ¥è©¢
DESIGN_YOUTUBE_KEYWORDS = [
    # Cursor + Designï¼ˆç°¡åŒ–ç‰ˆï¼‰
    "cursor design system",
    "cursor UI components",
    "cursor build UI",
    "cursor web design",
    "cursor figma plugin",
    
    # Figma Make/MCP + Designï¼ˆç°¡åŒ–ç‰ˆï¼‰
    "figma make design",
    "figma mcp plugin",
    "figma make icon",
    
    # Vibe Coding + Designï¼ˆç°¡åŒ–ç‰ˆï¼‰
    "vibe coding design",
    "vibe coding UI",
    
    # æ›´å…·é«”çš„æ§‹å»ºæ¡ˆä¾‹ï¼ˆä¿ç•™æœ€é‡è¦çš„ï¼‰
    "built with cursor design",
    "cursor build website",
    "cursor create design",
]

def crawl_youtube_design_examples():
    """çˆ¬å– YouTube ä¸Š 15 å€‹æœ€å—æ­¡è¿çš„è¨­è¨ˆç›¸é—œæ¡ˆä¾‹"""
    print("=" * 70)
    print("çˆ¬å– YouTube ä¸Šè¨­è¨ˆç›¸é—œçš„ vibe-coding æ¡ˆä¾‹")
    print("=" * 70)
    print(f"ç›®æ¨™: 15 å€‹æœ€å—æ­¡è¿çš„æ¡ˆä¾‹")
    print(f"å°ˆæ³¨: UI/UX Design, Design System, Web Design")
    print(f"è¦æ±‚: å¿…é ˆç¢ºå¯¦ä½¿ç”¨ vibe-coding å·¥å…·ï¼ˆCursor, Figma Make/MCP ç­‰ï¼‰")
    print()
    
    crawler = AIExamplesCrawler()
    
    # ä¿®æ”¹ YouTube æœç´¢æ™‚é–“ç¯„åœç‚ºæ›´é•·ï¼ˆç²å–æ›´å¤šå€™é¸ï¼‰
    original_search_youtube = crawler.search_youtube
    
    def search_youtube_extended(self, query: str, max_results: int = 10):
        """æœç´¢ YouTube å½±ç‰‡ï¼ˆæ“´å±•æ™‚é–“ç¯„åœä»¥ç²å–æ›´å¤šå€™é¸ï¼‰"""
        import os
        import requests
        import time
        
        url = "https://www.googleapis.com/youtube/v3/search"
        
        # éå» 6 å€‹æœˆï¼ˆç²å–æ›´å¤šå€™é¸ï¼Œç„¶å¾ŒæŒ‰è§€çœ‹æ•¸æ’åºï¼‰
        published_after = (datetime.now() - timedelta(days=180)).isoformat() + "Z"
        
        YOUTUBE_API_KEY = os.getenv("YOUTUBE_API_KEY", "")
        
        params = {
            'part': 'snippet',
            'q': query,
            'type': 'video',
            'order': 'viewCount',  # æŒ‰è§€çœ‹æ•¸æ’åº
            'maxResults': max_results,
            'publishedAfter': published_after,
            'key': YOUTUBE_API_KEY
        }
        
        # æ·»åŠ é‡è©¦æ©Ÿåˆ¶
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                response = requests.get(url, params=params, timeout=15)
                
                # è™•ç† 403 éŒ¯èª¤
                if response.status_code == 403:
                    error_data = response.json()
                    error_reason = error_data.get('error', {}).get('errors', [{}])[0].get('reason', '')
                    
                    if 'quotaExceeded' in error_reason or 'quota' in str(error_data).lower():
                        print(f"    âš ï¸  API é…é¡å·²ç”¨å®Œï¼Œè·³éæ­¤é—œéµå­—")
                        return []
                    elif attempt < max_retries - 1:
                        wait_time = retry_delay * (attempt + 1)
                        print(f"    â³ API 403 éŒ¯èª¤ï¼Œç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                        time.sleep(wait_time)
                        continue
                    else:
                        print(f"    âš ï¸  API 403 éŒ¯èª¤ï¼ˆå¯èƒ½æ˜¯æ¬Šé™å•é¡Œï¼‰ï¼Œè·³éæ­¤é—œéµå­—")
                        return []
                
                response.raise_for_status()
                data = response.json()
                
                # è™•ç†æˆåŠŸçš„æƒ…æ³
                results = []
                video_ids = []
                for item in data.get('items', []):
                    video_id = item['id']['videoId']
                    video_ids.append(video_id)
                
                # ç²å–è¦–é »çµ±è¨ˆä¿¡æ¯
                if video_ids:
                    stats_url = "https://www.googleapis.com/youtube/v3/videos"
                    stats_params = {
                        'part': 'statistics,snippet',
                        'id': ','.join(video_ids),
                        'key': YOUTUBE_API_KEY
                    }
                    stats_response = requests.get(stats_url, params=stats_params)
                    stats_response.raise_for_status()
                    stats_data = stats_response.json()
                    
                    stats_dict = {}
                    for item in stats_data.get('items', []):
                        stats_dict[item['id']] = item['statistics']
                    
                    for item in data.get('items', []):
                        video_id = item['id']['videoId']
                        snippet = item['snippet']
                        stats = stats_dict.get(video_id, {})
                        
                        results.append({
                            'title': snippet['title'],
                            'description': snippet['description'],
                            'url': f"https://www.youtube.com/watch?v={video_id}",
                            'thumbnail': snippet['thumbnails'].get('high', {}).get('url', ''),
                            'creator': snippet['channelTitle'],
                            'creator_url': f"https://www.youtube.com/channel/{snippet['channelId']}",
                            'platform': 'YouTube',
                            'published_at': snippet['publishedAt'],
                            'view_count': int(stats.get('viewCount', 0)),
                            'like_count': int(stats.get('likeCount', 0)),
                            'comment_count': int(stats.get('commentCount', 0))
                        })
                
                # æŒ‰è§€çœ‹æ•¸æ’åº
                results.sort(key=lambda x: x.get('view_count', 0), reverse=True)
                
                return results
                
            except requests.exceptions.RequestException as e:
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (attempt + 1)
                    print(f"    â³ è«‹æ±‚éŒ¯èª¤ï¼Œç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"    âš ï¸  YouTube search error after {max_retries} attempts: {e}")
                    return []
            except Exception as e:
                print(f"    âš ï¸  YouTube search error: {e}")
                return []
        
        return []  # å¦‚æœæ‰€æœ‰é‡è©¦éƒ½å¤±æ•—
    
    import types
    crawler.search_youtube = types.MethodType(search_youtube_extended, crawler)
    
    # æœå°‹ YouTubeï¼ˆæ·»åŠ å»¶é²ä»¥é¿å… API é™åˆ¶ï¼‰
    print("ğŸ” æœå°‹ YouTube...")
    print("   æ³¨æ„: æ¯å€‹é—œéµå­—ä¹‹é–“æœƒç­‰å¾… 2 ç§’ï¼Œé¿å…è§¸ç™¼ API é™åˆ¶")
    all_youtube_results = []
    
    import time
    
    for i, keyword in enumerate(DESIGN_YOUTUBE_KEYWORDS):
        print(f"  [{i+1}/{len(DESIGN_YOUTUBE_KEYWORDS)}] é—œéµå­—: {keyword}")
        
        # æ·»åŠ å»¶é²ï¼ˆç¬¬ä¸€å€‹é—œéµå­—ä¸éœ€è¦ï¼‰
        if i > 0:
            time.sleep(2)
        
        results = crawler.search_youtube(keyword, max_results=10)
        all_youtube_results.extend(results)
        print(f"    æ‰¾åˆ° {len(results)} å€‹çµæœ")
    
    # å»é‡ YouTube
    seen_youtube_urls = set()
    unique_youtube = []
    for result in all_youtube_results:
        if result['url'] not in seen_youtube_urls:
            seen_youtube_urls.add(result['url'])
            unique_youtube.append(result)
    
    # æŒ‰è§€çœ‹æ•¸æ’åº
    unique_youtube.sort(key=lambda x: x.get('view_count', 0), reverse=True)
    
    print(f"\nâœ… YouTube: æ‰¾åˆ° {len(unique_youtube)} å€‹å”¯ä¸€çµæœ")
    print(f"   å‰ 5 å€‹è§€çœ‹æ•¸: {[x.get('view_count', 0) for x in unique_youtube[:5]]}")
    
    # ä½¿ç”¨ AI åˆ†æï¼ˆå°ˆæ³¨æ–¼è¨­è¨ˆç›¸é—œä¸”å¿…é ˆä½¿ç”¨ vibe-coding å·¥å…·ï¼‰
    print("\nğŸ¤– AI åˆ†æä¸­...")
    print("   å°ˆæ³¨æ–¼: UI/UX Design, Design System, Web Design")
    print("   è¦æ±‚: å¿…é ˆç¢ºå¯¦ä½¿ç”¨ vibe-coding toolsï¼ˆå¦‚ Cursor, Figma Make/MCPï¼‰")
    print("   è¦æ±‚: å¿…é ˆæ˜¯å¯¦éš›æ§‹å»ºçš„é …ç›®ï¼Œä¸æ˜¯å–®ç´”è¨è«–")
    
    design_examples = []
    
    # åˆ†æ YouTubeï¼ˆå–å‰ 50 å€‹å€™é¸ï¼Œç„¶å¾Œç¯©é¸å‡ºç¬¦åˆæ¢ä»¶çš„ 15 å€‹ï¼‰
    candidates = unique_youtube[:50]  # å–å‰ 50 å€‹ä½œç‚ºå€™é¸
    
    import time
    
    for i, result in enumerate(candidates, 1):
        print(f"\n[{i}/{len(candidates)}] YouTube: {result['title'][:60]}...")
        print(f"    è§€çœ‹æ•¸: {result.get('view_count', 0):,}")
        
        # æ·»åŠ å»¶é²ä»¥é¿å…è¶…é API é…é¡ï¼ˆæ¯åˆ†é˜ 10 æ¬¡ = æ¯ 6 ç§’ä¸€æ¬¡ï¼‰
        if i > 1:
            time.sleep(7)  # ç­‰å¾… 7 ç§’ï¼Œç¢ºä¿ä¸æœƒè¶…éé…é¡
        
        try:
            # ä½¿ç”¨æ­£ç¢ºçš„æ–¹æ³•åç¨±
            analysis = crawler.analyze_with_ai(result)
            
            # æª¢æŸ¥æ˜¯å¦èˆ‡è¨­è¨ˆç›¸é—œ
            category_tags = analysis.get('category_tags', [])
            is_design_related = any(tag in ['Design', 'Design System', 'UI/UX', 'Web Design', 'UI', 'UX'] 
                                   for tag in category_tags)
            
            # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨äº†è¨­è¨ˆç›¸é—œçš„ vibe-coding å·¥å…·
            tools = analysis.get('ai_tools_used', [])
            has_design_tool = any(tool in ['Cursor', 'Figma', 'Figma Make', 'Figma MCP', 'Claude', 'ChatGPT', 'Lovable', 'v0'] 
                                for tool in tools)
            
            # æª¢æŸ¥æ˜¯å¦ç¢ºå¯¦æ§‹å»ºäº†é …ç›®ï¼ˆä¸æ˜¯å–®ç´”è¨è«–ï¼‰
            is_real_project = analysis.get('is_real_project', False)
            project_evidence = analysis.get('project_evidence', '').lower()
            has_build_evidence = any(keyword in project_evidence for keyword in [
                'build', 'create', 'made', 'generated', 'developed', 'constructed',
                'built a', 'built an', 'built the', 'created a', 'created an',
                'made a', 'made an', 'developed a', 'developed an'
            ])
            
            # æª¢æŸ¥æ¨™é¡Œå’Œæè¿°ä¸­æ˜¯å¦æœ‰æ§‹å»ºè­‰æ“š
            title_desc = (result.get('title', '') + ' ' + result.get('description', '')).lower()
            title_has_build = any(keyword in title_desc for keyword in [
                'build', 'create', 'made', 'built with', 'built a', 'built an',
                'created', 'made with', 'using cursor', 'using figma',
                'cursor build', 'figma make', 'figma mcp'
            ])
            
            # å¿…é ˆæ»¿è¶³æ‰€æœ‰æ¢ä»¶ï¼š
            # 1. è¨­è¨ˆç›¸é—œ OR ä½¿ç”¨äº†è¨­è¨ˆå·¥å…·
            # 2. ä½¿ç”¨äº† vibe-coding å·¥å…·
            # 3. æ˜¯å¯¦éš›é …ç›®ï¼ˆæœ‰æ§‹å»ºè­‰æ“šï¼‰
            meets_criteria = (
                (is_design_related or has_design_tool) and
                has_design_tool and
                (is_real_project or has_build_evidence or title_has_build)
            )
            
            if meets_criteria:
                # ä½¿ç”¨ process_content æ–¹æ³•å‰µå»ºç¤ºä¾‹
                example = crawler.process_content(result)
                
                # æª¢æŸ¥ process_content æ˜¯å¦è¿”å› Noneï¼ˆå¯èƒ½å› ç‚º relevance_score å¤ªä½ï¼‰
                if example is None:
                    print(f"  âš ï¸  process_content è¿”å› Noneï¼ˆå¯èƒ½ relevance_score < 6 æˆ–ä¸æ˜¯çœŸå¯¦é …ç›®ï¼‰")
                    # æ‰‹å‹•å‰µå»ºç¤ºä¾‹ï¼ˆå› ç‚ºæˆ‘å€‘å·²ç¶“é€šéäº†ç¯©é¸æ¢ä»¶ï¼‰
                    example = {
                        'title': result['title'],
                        'description': analysis.get('enhanced_description', result.get('description', '')),
                        'ai_tools_used': tools,
                        'category_tags': category_tags,
                        'source_platform': 'YouTube',
                        'original_url': result['url'],
                        'creator_name': result.get('creator', 'Unknown'),
                        'creator_link': result.get('creator_url', ''),
                        'thumbnail_url': result.get('thumbnail', ''),
                        'date_added': datetime.now().isoformat(),
                        'relevance_score': analysis.get('relevance_score', 7),
                        'build_complexity': analysis.get('build_complexity', 'Low-code'),
                        'is_no_code_low_code': analysis.get('is_no_code_low_code', False),
                        'project_name': analysis.get('project_name', result['title'][:50]),
                        'project_summary': analysis.get('project_summary', ''),
                        'project_evidence': analysis.get('project_evidence', ''),
                        'view_count': result.get('view_count', 0),
                        'like_count': result.get('like_count', 0),
                        'comment_count': result.get('comment_count', 0),
                        'primary_category': 'Design'
                    }
                
                example['source_platform'] = 'YouTube'
                example['view_count'] = result.get('view_count', 0)
                example['like_count'] = result.get('like_count', 0)
                example['comment_count'] = result.get('comment_count', 0)
                example['primary_category'] = 'Design'
                design_examples.append(example)
                print(f"  âœ… ç¬¦åˆæ¢ä»¶ï¼Œå·²åŠ å…¥ï¼ˆå·¥å…·: {tools}, åˆ†é¡: {category_tags}ï¼‰")
                
                # å¦‚æœå·²ç¶“æœ‰ 15 å€‹ç¬¦åˆæ¢ä»¶çš„æ¡ˆä¾‹ï¼Œå°±åœæ­¢
                if len(design_examples) >= 15:
                    print(f"\nâœ… å·²æ‰¾åˆ° 15 å€‹ç¬¦åˆæ¢ä»¶çš„æ¡ˆä¾‹ï¼Œåœæ­¢åˆ†æ")
                    break
            else:
                reasons = []
                if not is_design_related and not has_design_tool:
                    reasons.append("éè¨­è¨ˆç›¸é—œæˆ–æœªä½¿ç”¨è¨­è¨ˆå·¥å…·")
                if not has_design_tool:
                    reasons.append("æœªä½¿ç”¨ vibe-coding å·¥å…·")
                if not is_real_project and not has_build_evidence and not title_has_build:
                    reasons.append("ç„¡æ§‹å»ºè­‰æ“š")
                print(f"  âš ï¸  ä¸ç¬¦åˆæ¢ä»¶: {', '.join(reasons) if reasons else 'æœªçŸ¥'}")
                
        except Exception as e:
            error_msg = str(e)
            if "429" in error_msg or "quota" in error_msg.lower():
                # æå–é‡è©¦æ™‚é–“
                import re
                retry_match = re.search(r'retry in (\d+\.?\d*)s', error_msg)
                if retry_match:
                    retry_seconds = float(retry_match.group(1))
                    print(f"  â³ API é…é¡é™åˆ¶ï¼Œç­‰å¾… {int(retry_seconds)} ç§’å¾Œé‡è©¦...")
                    import time
                    time.sleep(min(retry_seconds + 2, 60))  # æœ€å¤šç­‰å¾… 60 ç§’
                    # é‡è©¦ä¸€æ¬¡
                    try:
                        analysis = crawler.analyze_with_ai(result)
                        # ç¹¼çºŒè™•ç†...
                        category_tags = analysis.get('category_tags', [])
                        is_design_related = any(tag in ['Design', 'Design System', 'UI/UX', 'Web Design', 'UI', 'UX'] 
                                               for tag in category_tags)
                        tools = analysis.get('ai_tools_used', [])
                        has_design_tool = any(tool in ['Cursor', 'Figma', 'Figma Make', 'Figma MCP', 'Claude', 'ChatGPT', 'Lovable', 'v0'] 
                                            for tool in tools)
                        is_real_project = analysis.get('is_real_project', False)
                        project_evidence = analysis.get('project_evidence', '').lower()
                        has_build_evidence = any(keyword in project_evidence for keyword in [
                            'build', 'create', 'made', 'generated', 'developed', 'constructed',
                            'built a', 'built an', 'built the', 'created a', 'created an',
                            'made a', 'made an', 'developed a', 'developed an'
                        ])
                        title_desc = (result.get('title', '') + ' ' + result.get('description', '')).lower()
                        title_has_build = any(keyword in title_desc for keyword in [
                            'build', 'create', 'made', 'built with', 'built a', 'built an',
                            'created', 'made with', 'using cursor', 'using figma',
                            'cursor build', 'figma make', 'figma mcp'
                        ])
                        meets_criteria = (
                            (is_design_related or has_design_tool) and
                            has_design_tool and
                            (is_real_project or has_build_evidence or title_has_build)
                        )
                        if meets_criteria:
                            example = crawler.process_content(result)
                            if example is None:
                                example = {
                                    'title': result['title'],
                                    'description': analysis.get('enhanced_description', result.get('description', '')),
                                    'ai_tools_used': tools,
                                    'category_tags': category_tags,
                                    'source_platform': 'YouTube',
                                    'original_url': result['url'],
                                    'creator_name': result.get('creator', 'Unknown'),
                                    'creator_link': result.get('creator_url', ''),
                                    'thumbnail_url': result.get('thumbnail', ''),
                                    'date_added': datetime.now().isoformat(),
                                    'relevance_score': analysis.get('relevance_score', 7),
                                    'build_complexity': analysis.get('build_complexity', 'Low-code'),
                                    'is_no_code_low_code': analysis.get('is_no_code_low_code', False),
                                    'project_name': analysis.get('project_name', result['title'][:50]),
                                    'project_summary': analysis.get('project_summary', ''),
                                    'project_evidence': analysis.get('project_evidence', ''),
                                    'view_count': result.get('view_count', 0),
                                    'like_count': result.get('like_count', 0),
                                    'comment_count': result.get('comment_count', 0),
                                    'primary_category': 'Design'
                                }
                            example['source_platform'] = 'YouTube'
                            example['view_count'] = result.get('view_count', 0)
                            example['like_count'] = result.get('like_count', 0)
                            example['comment_count'] = result.get('comment_count', 0)
                            example['primary_category'] = 'Design'
                            design_examples.append(example)
                            print(f"  âœ… ç¬¦åˆæ¢ä»¶ï¼Œå·²åŠ å…¥ï¼ˆå·¥å…·: {tools}, åˆ†é¡: {category_tags}ï¼‰")
                            if len(design_examples) >= 15:
                                print(f"\nâœ… å·²æ‰¾åˆ° 15 å€‹ç¬¦åˆæ¢ä»¶çš„æ¡ˆä¾‹ï¼Œåœæ­¢åˆ†æ")
                                break
                        else:
                            print(f"  âš ï¸  é‡è©¦å¾Œä»ä¸ç¬¦åˆæ¢ä»¶")
                    except Exception as retry_e:
                        print(f"  âŒ é‡è©¦å¾Œä»å¤±æ•—: {retry_e}")
                else:
                    print(f"  â³ API é…é¡é™åˆ¶ï¼Œè·³éæ­¤æ¡ˆä¾‹")
            else:
                print(f"  âŒ åˆ†æéŒ¯èª¤: {e}")
                import traceback
                traceback.print_exc()
    
    print(f"\nâœ… æ‰¾åˆ° {len(design_examples)} å€‹ç¬¦åˆæ¢ä»¶çš„è¨­è¨ˆæ¡ˆä¾‹")
    
    # è¼‰å…¥ç¾æœ‰æ•¸æ“š
    data_file = Path(__file__).parent.parent / "found_examples_latest.json"
    existing_examples = []
    if data_file.exists():
        with open(data_file, 'r', encoding='utf-8') as f:
            existing_examples = json.load(f)
        print(f"ğŸ“‚ è¼‰å…¥ç¾æœ‰æ•¸æ“š: {len(existing_examples)} å€‹æ¡ˆä¾‹")
    
    # åˆä½µæ•¸æ“šï¼ˆä¿ç•™ç¾æœ‰ + æ–°å¢è¨­è¨ˆç›¸é—œï¼‰
    all_examples = existing_examples + design_examples
    
    # å»é‡ï¼ˆæ ¹æ“š URLï¼‰
    seen_urls = set()
    unique_all = []
    for ex in all_examples:
        url = ex.get('original_url', '')
        if url and url not in seen_urls:
            seen_urls.add(url)
            unique_all.append(ex)
    
    # ç‚ºæ–°æ¡ˆä¾‹æ·»åŠ åˆ†é¡
    for ex in design_examples:
        if 'primary_category' not in ex:
            ex['primary_category'] = 'Design'
    
    # æ’åºï¼ˆYouTube å„ªå…ˆï¼Œç„¶å¾ŒæŒ‰è§€çœ‹æ•¸ï¼‰
    unique_all.sort(key=lambda x: (
        0 if x.get('source_platform') == 'YouTube' else 1,
        -x.get('view_count', 0) if x.get('source_platform') == 'YouTube' else 0,
        -x.get('relevance_score', 0)
    ))
    
    # ä¿å­˜
    with open(data_file, 'w', encoding='utf-8') as f:
        json.dump(unique_all, f, indent=2, ensure_ascii=False)
    
    youtube_count = len([x for x in design_examples if x.get('source_platform') == 'YouTube'])
    
    print(f"\n{'='*70}")
    print("âœ… å®Œæˆï¼")
    print(f"{'='*70}")
    print(f"  æ–°å¢ YouTube è¨­è¨ˆæ¡ˆä¾‹: {youtube_count}")
    print(f"  ç¸½æ¡ˆä¾‹æ•¸: {len(unique_all)}")
    print(f"\næ•¸æ“šå·²ä¿å­˜åˆ°: {data_file}")
    
    if youtube_count < 15:
        print(f"\nâš ï¸  åªæ‰¾åˆ° {youtube_count} å€‹ç¬¦åˆæ¢ä»¶çš„æ¡ˆä¾‹ï¼ˆç›®æ¨™: 15 å€‹ï¼‰")
        print(f"   å¯èƒ½åŸå› :")
        print(f"   - API é…é¡é™åˆ¶")
        print(f"   - ç¬¦åˆæ¢ä»¶çš„æ¡ˆä¾‹è¼ƒå°‘")
        print(f"   - é—œéµå­—éœ€è¦èª¿æ•´")

if __name__ == "__main__":
    crawl_youtube_design_examples()

